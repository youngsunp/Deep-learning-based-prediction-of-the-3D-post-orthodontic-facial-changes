{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_height = 128 \n",
    "image_width = 128 \n",
    "real_width = 394\n",
    "real_height = 394\n",
    "real_depth = 234\n",
    "condition_shape = 7\n",
    "\n",
    "total_epoch = 1\n",
    "batch_size = 1\n",
    "shuffle_buffer_size = 1\n",
    "\n",
    "test_input_dir = \"D:/new202103/test_input/test1_64\" \n",
    "test_output_dir = \"D:/new202103/test_output/test1_64\"\n",
    "test_condition_dir = \"D:/new202103/movement_grad2/test1/9\"\n",
    "result_dir =  './output/result/5/gradient/test1' \n",
    "ckpt_dir = './output/checkpoint/5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Class for batch normalization node\n",
    "\n",
    "class batch_norm(object):\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "            self.momentum = momentum\n",
    "\n",
    "            self.name = name\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "\n",
    "                                            decay=self.momentum,\n",
    "\n",
    "                                            updates_collections=None,\n",
    "\n",
    "                                            epsilon=self.epsilon,\n",
    "\n",
    "                                            scale=True,\n",
    "\n",
    "                                            is_training=train,\n",
    "\n",
    "                                            scope=self.name,\n",
    "\n",
    "                                            reuse=tf.AUTO_REUSE     # if tensorflow vesrion < 1.4, delete this line\n",
    "\n",
    "                                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# leaky relu function\n",
    "\n",
    "def lrelu(X, leak=0.2):\n",
    "\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "\n",
    "    return f1 * X + f2 * tf.abs(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Pix2Pix:\n",
    "\n",
    "        \n",
    "    def generate(self, input_img, con_input):\n",
    "        dummy = np.ones((1,64,64,1))\n",
    "        img_concat = tf.concat([con_input[0,0,0,0]*dummy, con_input[0,0,0,1]*dummy,con_input[0,0,0,2]*dummy,con_input[0,0,0,3]*dummy,con_input[0,0,0,4]*dummy,con_input[0,0,0,5]*dummy,con_input[0,0,0,6]*dummy], axis=3)\n",
    "        \n",
    "        \n",
    "        h1 = h1_ = tf.nn.conv2d(input_img, self.G_W1, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "        h1 = tf.concat([h1, img_concat], axis=3)\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.G_W2, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h2 = h2_ = self.G_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.G_W3, strides=[1, 2, 2, 1], padding='SAME') \n",
    "\n",
    "        h3 = h3_ = self.G_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.nn.conv2d(h3, self.G_W4, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h4 = h4_ = self.G_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.nn.conv2d(h4, self.G_W5, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h5 = h5_ = self.G_bn5(h5)\n",
    "\n",
    "        h5 = lrelu(h5)\n",
    "\n",
    "\n",
    "\n",
    "        h6 = tf.nn.conv2d(h5, self.G_W6, strides=[1, 2, 2, 1], padding='SAME') \n",
    "\n",
    "        h6 = h6_ = self.G_bn6(h6)\n",
    "\n",
    "        h6 = lrelu(h6)\n",
    "\n",
    "\n",
    "\n",
    "        h7 = tf.nn.conv2d(h6, self.G_W7, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h7 = h7_ = self.G_bn7(h7)\n",
    "\n",
    "        h7 = lrelu(h7)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        h9 = tf.nn.conv2d_transpose(h7, self.G_W9, output_shape=[batch_size, 2, 2, self.ch_G9], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h9 = tf.nn.dropout(self.G_bn9(h9), keep_prob=self.keep_prob)\n",
    "\n",
    "        h9 = tf.nn.relu(h9)\n",
    "\n",
    "        h9 = tf.concat([h9, h6_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h10 = tf.nn.conv2d_transpose(h9, self.G_W10, output_shape=[batch_size, 4, 4, self.ch_G10], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h10 = tf.nn.dropout(self.G_bn10(h10), keep_prob=self.keep_prob)\n",
    "\n",
    "        h10 = tf.nn.relu(h10)\n",
    "\n",
    "        h10 = tf.concat([h10, h5_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h11 = tf.nn.conv2d_transpose(h10, self.G_W11, output_shape=[batch_size, 8, 8, self.ch_G11], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h11 = tf.nn.dropout(self.G_bn11(h11), keep_prob=self.keep_prob)\n",
    "\n",
    "        h11 = tf.nn.relu(h11)\n",
    "\n",
    "        h11 = tf.concat([h11, h4_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h12 = tf.nn.conv2d_transpose(h11, self.G_W12, output_shape=[batch_size, 16, 16, self.ch_G12], strides=[1, 2, 2, 1]) \n",
    "\n",
    "        h12 = self.G_bn12(h12)\n",
    "\n",
    "        h12 = tf.nn.relu(h12)\n",
    "\n",
    "        h12 = tf.concat([h12, h3_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h13 = tf.nn.conv2d_transpose(h12, self.G_W13, output_shape=[batch_size, 32, 32, self.ch_G13], strides=[1, 2, 2, 1])  \n",
    "        \n",
    "        h13 = self.G_bn13(h13)\n",
    "\n",
    "        h13 = tf.nn.relu(h13)\n",
    "\n",
    "        h13 = tf.concat([h13, h2_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h14 = tf.nn.conv2d_transpose(h13, self.G_W14, output_shape=[batch_size, 64, 64, self.ch_G14], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h14 = self.G_bn14(h14)\n",
    "\n",
    "        h14 = tf.nn.relu(h14)\n",
    "\n",
    "        h14 = tf.concat([h14, h1_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        h16 = tf.nn.conv2d_transpose(h14, self.G_W16, output_shape=[batch_size, 128, 128, self.ch_G16], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h16 = tf.nn.tanh(h16)\n",
    "\n",
    "\n",
    "        return h16\n",
    "\n",
    "    \n",
    "    \n",
    "    # Network Parameters\n",
    "\n",
    "    def __init__(self, sess, batch_size):\n",
    "\n",
    "        self.learning_rate = 0.00001  \n",
    "\n",
    "\n",
    "\n",
    "        self.sess = sess\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.keep_prob = 0.5\n",
    "\n",
    "        self.input_image_shape = [image_height, image_width, 1] \n",
    "        self.input_condition_shape = [1,1,condition_shape] \n",
    "        self.output_image_shape = [image_height, image_width, 1]\n",
    "\n",
    "        self.l1_weight = 1000.0 \n",
    "\n",
    "\n",
    "\n",
    "        '''channels'''\n",
    "\n",
    "        # Gen_Encoding\n",
    "\n",
    "        self.ch_G0 = 1\n",
    "\n",
    "        self.ch_G1 = 64-condition_shape \n",
    "\n",
    "        self.ch_G2 = 128\n",
    "\n",
    "        self.ch_G3 = 256\n",
    "\n",
    "        self.ch_G4 = 512\n",
    "\n",
    "        self.ch_G5 = 512\n",
    "\n",
    "        self.ch_G6 = 512\n",
    "\n",
    "        self.ch_G7 = 512\n",
    "        \n",
    "        self.ch_condition = condition_shape\n",
    "\n",
    "\n",
    "        # Gen_Decoding\n",
    "\n",
    "        self.ch_G9 = 512\n",
    "\n",
    "        self.ch_G10 = 512\n",
    "\n",
    "        self.ch_G11 = 512\n",
    "\n",
    "        self.ch_G12 = 256\n",
    "\n",
    "        self.ch_G13 = 128\n",
    "        \n",
    "        self.ch_G14 = 64\n",
    "\n",
    "        self.ch_G16 = 1\n",
    "\n",
    "        # Discrim\n",
    "\n",
    "        self.ch_D0 = 2+condition_shape \n",
    "\n",
    "        self.ch_D1 = 64\n",
    "\n",
    "        self.ch_D2 = 128\n",
    "\n",
    "        self.ch_D3 = 256\n",
    "\n",
    "        self.ch_D4 = 512\n",
    "\n",
    "        self.ch_D5 = 1\n",
    "\n",
    "\n",
    "\n",
    "        '''parameters'''\n",
    "\n",
    "        # Gen_encoding\n",
    "\n",
    "        self.G_W1 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G0, self.ch_G1], stddev=0.02), name=\"G_W1\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W2 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G1+7, self.ch_G2], stddev=0.02), name='G_W2')\n",
    "\n",
    "        self.G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W3 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G2, self.ch_G3], stddev=0.02), name='G_W3')\n",
    "\n",
    "        self.G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W4 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G3, self.ch_G4], stddev=0.02), name='G_W4')\n",
    "\n",
    "        self.G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W5 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G4, self.ch_G5], stddev=0.02), name='G_W5')\n",
    "\n",
    "        self.G_bn5 = batch_norm(name=\"G_bn5\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W6 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G5, self.ch_G6], stddev=0.02), name='G_W6')\n",
    "\n",
    "        self.G_bn6 = batch_norm(name=\"G_bn6\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W7 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G6, self.ch_G7], stddev=0.02), name='G_W7')\n",
    "\n",
    "        self.G_bn7 = batch_norm(name=\"G_bn7\")\n",
    "\n",
    "\n",
    "\n",
    "        # Gen_Decoding\n",
    "\n",
    "        self.G_W9 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G9, self.ch_G7], stddev=0.02), name='G_W9')\n",
    "\n",
    "        self.G_bn9 = batch_norm(name=\"G_bn9\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W10 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G10, self.ch_G9 + self.ch_G6], stddev=0.02), name='G_W10')\n",
    "\n",
    "        self.G_bn10 = batch_norm(name=\"G_bn10\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W11 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G11, self.ch_G10 + self.ch_G5], stddev=0.02), name='G_W11')\n",
    "\n",
    "        self.G_bn11 = batch_norm(name=\"G_bn11\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W12 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G12, self.ch_G11 + self.ch_G4], stddev=0.02), name='G_W12')\n",
    "\n",
    "        self.G_bn12 = batch_norm(name=\"G_bn12\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W13 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G13, self.ch_G12 + self.ch_G3], stddev=0.02), name='G_W13')\n",
    "\n",
    "        self.G_bn13 = batch_norm(name=\"G_bn13\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W14 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G14, self.ch_G13 + self.ch_G2], stddev=0.02), name='G_W14')\n",
    "\n",
    "        self.G_bn14 = batch_norm(name=\"G_bn14\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W16 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G16, self.ch_G14 + self.ch_G1], stddev=0.02), name='G_W16')\n",
    "\n",
    "\n",
    "\n",
    "        # Discrim\n",
    "\n",
    "        self.D_W1 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D0, self.ch_D1], stddev=0.02), name='D_W1')\n",
    "\n",
    "        self.D_bn1 = batch_norm(name=\"D_bn1\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W2 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D1, self.ch_D2], stddev=0.02), name='D_W2')\n",
    "\n",
    "        self.D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W3 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D2, self.ch_D3], stddev=0.02), name='D_W3')\n",
    "\n",
    "        self.D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W4 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D3, self.ch_D4], stddev=0.02), name='D_W4')\n",
    "\n",
    "        self.D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W5 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D4, self.ch_D5], stddev=0.02), name='D_W5')\n",
    "\n",
    "\n",
    "\n",
    "        self.gen_params = [\n",
    "\n",
    "            self.G_W1,\n",
    "\n",
    "            self.G_W2,\n",
    "\n",
    "            self.G_W3,\n",
    "\n",
    "            self.G_W4,\n",
    "\n",
    "            self.G_W5,\n",
    "\n",
    "            self.G_W6,\n",
    "\n",
    "            self.G_W7,\n",
    "\n",
    "            self.G_W9,\n",
    "\n",
    "            self.G_W10,\n",
    "\n",
    "            self.G_W11,\n",
    "\n",
    "            self.G_W12,\n",
    "\n",
    "            self.G_W13,\n",
    "\n",
    "            self.G_W14,\n",
    "\n",
    "            self.G_W16\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        self.discrim_params = [\n",
    "\n",
    "            self.D_W1,\n",
    "\n",
    "            self.D_W2,\n",
    "\n",
    "            self.D_W3,\n",
    "\n",
    "            self.D_W4,\n",
    "\n",
    "            self.D_W5\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "\n",
    "    def discriminate(self, input_img, target, condition):\n",
    "        \n",
    "\n",
    "        dummy = np.ones((1,image_height,image_width,1))\n",
    "        if condition_shape==7:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy,condition[0,0,0,4]*dummy,condition[0,0,0,5]*dummy,condition[0,0,0,6]*dummy], axis=3)\n",
    "        if condition_shape==6:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy,condition[0,0,0,4]*dummy,condition[0,0,0,5]*dummy], axis=3)\n",
    "        if condition_shape==4:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy], axis=3)\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        h1 = tf.nn.conv2d(img_concat, self.D_W1, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h1 = self.D_bn1(h1)\n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.D_W2, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h2 = self.D_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.D_W3, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h3 = self.D_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.pad(h3, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')  \n",
    "\n",
    "        h4 = tf.nn.conv2d(h4, self.D_W4, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "        h4 = self.D_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.pad(h4, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')  \n",
    "\n",
    "        h5 = tf.nn.conv2d(h5, self.D_W5, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "        h5 = tf.nn.sigmoid(h5)\n",
    "\n",
    "\n",
    "\n",
    "        return h5\n",
    "\n",
    "\n",
    "\n",
    "    # Method for generating the fake images\n",
    "\n",
    "    def sample_generator(self, input_image, cond_input, batch_size=1):\n",
    "\n",
    "        input_img = tf.placeholder(tf.float32, [batch_size] + self.input_image_shape)\n",
    "        con_input = tf.placeholder(tf.float32, [batch_size] + self.input_condition_shape)\n",
    "        \n",
    "        dummy = np.ones((1,64,64,1))\n",
    "        img_concat = tf.concat([con_input[0,0,0,0]*dummy, con_input[0,0,0,1]*dummy,con_input[0,0,0,2]*dummy,con_input[0,0,0,3]*dummy,con_input[0,0,0,4]*dummy,con_input[0,0,0,5]*dummy,con_input[0,0,0,6]*dummy], axis=3)\n",
    "        \n",
    "        \n",
    "        h1 = h1_ = tf.nn.conv2d(input_img, self.G_W1, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "        h1 = tf.concat([h1, img_concat], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.G_W2, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h2 = h2_ = self.G_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.G_W3, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h3 = h3_ = self.G_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.nn.conv2d(h3, self.G_W4, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h4 = h4_ = self.G_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.nn.conv2d(h4, self.G_W5, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h5 = h5_ = self.G_bn5(h5)\n",
    "\n",
    "        h5 = lrelu(h5)\n",
    "\n",
    "\n",
    "\n",
    "        h6 = tf.nn.conv2d(h5, self.G_W6, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h6 = h6_ = self.G_bn6(h6)\n",
    "\n",
    "        h6 = lrelu(h6)\n",
    "\n",
    "\n",
    "\n",
    "        h7 = tf.nn.conv2d(h6, self.G_W7, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h7 = h7_ = self.G_bn7(h7)\n",
    "\n",
    "        h7 = lrelu(h7)\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "        h9 = tf.nn.conv2d_transpose(h7, self.G_W9, output_shape=[batch_size, 2, 2, self.ch_G9], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h9 = tf.nn.dropout(self.G_bn9(h9), keep_prob=self.keep_prob)\n",
    "\n",
    "        h9 = tf.nn.relu(h9)\n",
    "\n",
    "        h9 = tf.concat([h9, h6_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h10 = tf.nn.conv2d_transpose(h9, self.G_W10, output_shape=[batch_size, 4, 4, self.ch_G10], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h10 = tf.nn.dropout(self.G_bn10(h10), keep_prob=self.keep_prob)\n",
    "\n",
    "        h10 = tf.nn.relu(h10)\n",
    "\n",
    "        h10 = tf.concat([h10, h5_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h11 = tf.nn.conv2d_transpose(h10, self.G_W11, output_shape=[batch_size, 8, 8, self.ch_G11], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h11 = tf.nn.dropout(self.G_bn11(h11), keep_prob=self.keep_prob)\n",
    "\n",
    "        h11 = tf.nn.relu(h11)\n",
    "\n",
    "        h11 = tf.concat([h11, h4_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h12 = tf.nn.conv2d_transpose(h11, self.G_W12, output_shape=[batch_size, 16, 16, self.ch_G12], strides=[1, 2, 2, 1])  \n",
    "        \n",
    "        h12 = self.G_bn12(h12)\n",
    "\n",
    "        h12 = tf.nn.relu(h12)\n",
    "\n",
    "        h12 = tf.concat([h12, h3_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h13 = tf.nn.conv2d_transpose(h12, self.G_W13, output_shape=[batch_size, 32, 32, self.ch_G13], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h13 = self.G_bn13(h13)\n",
    "\n",
    "        h13 = tf.nn.relu(h13)\n",
    "\n",
    "        h13 = tf.concat([h13, h2_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h14 = tf.nn.conv2d_transpose(h13, self.G_W14, output_shape=[batch_size, 64, 64, self.ch_G14], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h14 = self.G_bn14(h14)\n",
    "\n",
    "        h14 = tf.nn.relu(h14)\n",
    "\n",
    "        h14 = tf.concat([h14, h1_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        h16 = tf.nn.conv2d_transpose(h14, self.G_W16, output_shape=[batch_size, 128, 128, self.ch_G16], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h16 = tf.nn.tanh(h16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        generated_samples = self.sess.run(h16, feed_dict={input_img: input_image, con_input: cond_input})\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return generated_samples\n",
    "\n",
    "\n",
    "\n",
    "    # Train Generator and return the loss\n",
    "\n",
    "    def train_gen(self, input_img, target_img, con_input):\n",
    "\n",
    "        \n",
    "\n",
    "        _, loss_val_GAN, loss_val_L1 = self.sess.run([self.train_op_gen, self.G_loss_GAN, self.G_loss_L1], feed_dict={self.input_img: input_img, self.target_img: target_img, self.input_con: con_input})\n",
    "\n",
    "      \n",
    "\n",
    "        return loss_val_GAN, loss_val_L1\n",
    "\n",
    "\n",
    "\n",
    "    # Train Discriminator and return the loss\n",
    "\n",
    "    def train_discrim(self, input_img, target_img, input_condition):\n",
    "\n",
    "        _, loss_val_D = self.sess.run([self.train_op_discrim, self.D_loss], feed_dict={self.input_img: input_img, self.target_img: target_img, self.input_con: input_condition})\n",
    "\n",
    "        return loss_val_D\n",
    "    \n",
    "\n",
    "    \n",
    "    # Build the Network\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        self.input_con = tf.placeholder(tf.float32, [self.batch_size] + self.input_condition_shape)\n",
    "        self.input_img = tf.placeholder(tf.float32, [self.batch_size] + self.input_image_shape)\n",
    "        \n",
    "\n",
    "        self.target_img = tf.placeholder(tf.float32, [self.batch_size] + self.output_image_shape)\n",
    "\n",
    "\n",
    "        gen_img = self.generate(self.input_img, self.input_con)\n",
    "\n",
    "\n",
    "\n",
    "        d_real = self.discriminate(self.input_img, self.target_img, self.input_con)\n",
    "\n",
    "        d_fake = self.discriminate(self.input_img, gen_img, self.input_con)\n",
    "\n",
    "\n",
    "\n",
    "        self.D_loss = tf.reduce_mean(tf.square(1-d_real) + tf.square(d_fake))\n",
    "      \n",
    "\n",
    "        self.G_loss_GAN = tf.reduce_mean(tf.square(1-d_fake))\n",
    "        \n",
    "        self.G_loss_L1 = tf.reduce_mean(tf.abs(self.target_img - gen_img))\n",
    "\n",
    "      \n",
    "        self.G_loss = self.G_loss_GAN + self.G_loss_L1 * self.l1_weight\n",
    "\n",
    "       \n",
    "        self.train_op_discrim = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5).minimize(self.D_loss, var_list=self.discrim_params)\n",
    "\n",
    "        self.train_op_gen = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5).minimize(self.G_loss, var_list=self.gen_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\young\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-4e02a4cb73b6>:129: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-c223dbe6d437>:15: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./output/checkpoint/5\\model_epoch005\n",
      "Generating...\n",
      "WARNING:tensorflow:From <ipython-input-3-c223dbe6d437>:55: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "2Generating...\n",
      "3Generating...\n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = Pix2Pix(sess, batch_size)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "else:\n",
    "    sys.exit(\"There is no trained model\")\n",
    "\n",
    "print('Generating...')\n",
    "\n",
    "# test data -> dataset \n",
    "test_input_path = []\n",
    "test_output_path = []\n",
    "test_condition_path = []\n",
    "\n",
    "features_list = os.listdir(test_input_dir)\n",
    "for feature_name in features_list :\n",
    "    x = '%s/%s' %(test_input_dir, feature_name)\n",
    "    test_input_path.append(x)\n",
    "    \n",
    "labels_list = os.listdir(test_output_dir)\n",
    "for label_name in labels_list :\n",
    "    x = '%s/%s' %(test_output_dir, label_name)\n",
    "    test_output_path.append(x)\n",
    "    \n",
    "condition_list = os.listdir(test_condition_dir)\n",
    "for condition_name in condition_list :\n",
    "    x = '%s/%s' %(test_condition_dir, condition_name)\n",
    "    test_condition_path.append(x)\n",
    "\n",
    "def read_path(inputpath, outputpath, conditionpath):\n",
    "\n",
    "    x = np.load(inputpath.decode(), allow_pickle = True) \n",
    "    y = np.load(outputpath.decode(), allow_pickle = True)\n",
    "    z = np.load(conditionpath.decode(), allow_pickle = True)\n",
    "    return x.astype(np.float64), y.astype(np.float64), z.astype(np.float64)\n",
    "    \n",
    "features, labels, conditions = (test_input_path, test_output_path, test_condition_path)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels, conditions))\n",
    "dataset = dataset.map(lambda features, labels, conditions: tuple(tf.py_func(read_path, [features,labels, conditions], [tf.float64, tf.float64, tf.float64])))\n",
    "dataset = dataset.batch(batch_size=batch_size).repeat(count=total_epoch).shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "feature, label, condition = iter.get_next()\n",
    "\n",
    "\n",
    "print('2Generating...')\n",
    "\n",
    "for i in range(len(test_input_path)) :\n",
    "    t_xs, t_ys, t_zs = sess.run([feature, label, condition])\n",
    "    t_xs = np.reshape(t_xs,[1,128,128,1])\n",
    "    t_zs = np.reshape(t_zs,[1,1,1,condition_shape])\n",
    "    t_ys = np.reshape(t_ys,[1,128,128,1])\n",
    "    generated_samples = model.sample_generator(t_xs, t_zs, batch_size=batch_size)\n",
    "    \n",
    "    real_initial = t_xs[0,:,:,0]\n",
    "    real_final = t_ys[0,:,:,0]\n",
    "    generated_samples = generated_samples[0,:,:,0]\n",
    "    print('3Generating...')\n",
    "    \n",
    "    resize_real_initial = np.zeros(shape=(real_width,real_height))\n",
    "    resize_real_final = np.zeros(shape=(real_width,real_height))\n",
    "    resize_generated_samples = np.zeros(shape=(real_width,real_height))\n",
    "\n",
    "    for W in range(real_width):\n",
    "        for H in range(real_height):\n",
    "            new_width = int( W * real_initial.shape[0] / real_width )\n",
    "            new_height = int( H * real_initial.shape[1] / real_height )\n",
    "            resize_real_initial[W,H] = real_initial[new_width,new_height]\n",
    "            resize_real_final[W,H] = real_final[new_width,new_height]\n",
    "            resize_generated_samples[W,H] = generated_samples[new_width,new_height]\n",
    "            \n",
    "    output3d_real_initial = np.zeros((real_width,real_depth,real_height))\n",
    "    output3d_real_final = np.zeros((real_width,real_depth,real_height))\n",
    "    output3d_generated_samples = np.zeros((real_width,real_depth,real_height))\n",
    "    for q in range(real_width) :\n",
    "        for r in range(real_height) :\n",
    "            if bool(resize_real_initial[q,r]) :\n",
    "                output3d_real_initial[q,0:int(resize_real_initial[q,r]*real_depth),r] = 1 \n",
    "            if bool(resize_real_final[q,r]) :\n",
    "                output3d_real_final[q,0:int(resize_real_final[q,r]*real_depth),r] = 1 \n",
    "            if bool(resize_real_initial[q,r]) :\n",
    "                output3d_generated_samples[q,0:int(resize_generated_samples[q,r]*real_depth),r] = 1 \n",
    "\n",
    " \n",
    "    np.save(result_dir+'/generated{}.npy'.format(i),output3d_generated_samples)\n",
    "\n",
    "print('finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
