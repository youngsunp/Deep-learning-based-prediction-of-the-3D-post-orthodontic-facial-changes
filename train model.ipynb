{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_height = 128 \n",
    "image_width = 128 \n",
    "total_epoch = 21\n",
    "training_iters = 13440 \n",
    "batch_size = 1 \n",
    "total_batch = training_iters \n",
    "shuffle_buffer_size = training_iters\n",
    "train_input_dir = \"D:/new202103/input\" \n",
    "train_output_dir = \"D:/new202103/output\" \n",
    "train_condition_dir = \"D:/new202103/newcondition(1by7)\"\n",
    "condition_shape = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Class for batch normalization node\n",
    "\n",
    "class batch_norm(object):\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "            self.momentum = momentum\n",
    "\n",
    "            self.name = name\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "\n",
    "                                            decay=self.momentum,\n",
    "\n",
    "                                            updates_collections=None,\n",
    "\n",
    "                                            epsilon=self.epsilon,\n",
    "\n",
    "                                            scale=True,\n",
    "\n",
    "                                            is_training=train,\n",
    "\n",
    "                                            scope=self.name,\n",
    "\n",
    "                                            reuse=tf.AUTO_REUSE     # if tensorflow vesrion < 1.4, delete this line\n",
    "\n",
    "                                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# leaky relu function\n",
    "\n",
    "def lrelu(X, leak=0.2):\n",
    "\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "\n",
    "    return f1 * X + f2 * tf.abs(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Pix2Pix:\n",
    "\n",
    "        \n",
    "    def generate(self, input_img, con_input):\n",
    "        dummy = np.ones((1,64,64,1))\n",
    "        img_concat = tf.concat([con_input[0,0,0,0]*dummy, con_input[0,0,0,1]*dummy,con_input[0,0,0,2]*dummy,con_input[0,0,0,3]*dummy,con_input[0,0,0,4]*dummy,con_input[0,0,0,5]*dummy,con_input[0,0,0,6]*dummy], axis=3)\n",
    "        \n",
    "        \n",
    "        h1 = h1_ = tf.nn.conv2d(input_img, self.G_W1, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "        h1 = tf.concat([h1, img_concat], axis=3)\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.G_W2, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h2 = h2_ = self.G_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.G_W3, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h3 = h3_ = self.G_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.nn.conv2d(h3, self.G_W4, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h4 = h4_ = self.G_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.nn.conv2d(h4, self.G_W5, strides=[1, 2, 2, 1], padding='SAME') \n",
    "\n",
    "        h5 = h5_ = self.G_bn5(h5)\n",
    "\n",
    "        h5 = lrelu(h5)\n",
    "\n",
    "\n",
    "\n",
    "        h6 = tf.nn.conv2d(h5, self.G_W6, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h6 = h6_ = self.G_bn6(h6)\n",
    "\n",
    "        h6 = lrelu(h6)\n",
    "\n",
    "\n",
    "\n",
    "        h7 = tf.nn.conv2d(h6, self.G_W7, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h7 = h7_ = self.G_bn7(h7)\n",
    "\n",
    "        h7 = lrelu(h7)        \n",
    "\n",
    "\n",
    "\n",
    "        h9 = tf.nn.conv2d_transpose(h7, self.G_W9, output_shape=[batch_size, 2, 2, self.ch_G9], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h9 = tf.nn.dropout(self.G_bn9(h9), keep_prob=self.keep_prob)\n",
    "\n",
    "        h9 = tf.nn.relu(h9)\n",
    "\n",
    "        h9 = tf.concat([h9, h6_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h10 = tf.nn.conv2d_transpose(h9, self.G_W10, output_shape=[batch_size, 4, 4, self.ch_G10], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h10 = tf.nn.dropout(self.G_bn10(h10), keep_prob=self.keep_prob)\n",
    "\n",
    "        h10 = tf.nn.relu(h10)\n",
    "\n",
    "        h10 = tf.concat([h10, h5_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h11 = tf.nn.conv2d_transpose(h10, self.G_W11, output_shape=[batch_size, 8, 8, self.ch_G11], strides=[1, 2, 2, 1]) \n",
    "\n",
    "        h11 = tf.nn.dropout(self.G_bn11(h11), keep_prob=self.keep_prob)\n",
    "\n",
    "        h11 = tf.nn.relu(h11)\n",
    "\n",
    "        h11 = tf.concat([h11, h4_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h12 = tf.nn.conv2d_transpose(h11, self.G_W12, output_shape=[batch_size, 16, 16, self.ch_G12], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h12 = self.G_bn12(h12)\n",
    "\n",
    "        h12 = tf.nn.relu(h12)\n",
    "\n",
    "        h12 = tf.concat([h12, h3_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h13 = tf.nn.conv2d_transpose(h12, self.G_W13, output_shape=[batch_size, 32, 32, self.ch_G13], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h13 = self.G_bn13(h13)\n",
    "\n",
    "        h13 = tf.nn.relu(h13)\n",
    "\n",
    "        h13 = tf.concat([h13, h2_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h14 = tf.nn.conv2d_transpose(h13, self.G_W14, output_shape=[batch_size, 64, 64, self.ch_G14], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h14 = self.G_bn14(h14)\n",
    "\n",
    "        h14 = tf.nn.relu(h14)\n",
    "\n",
    "        h14 = tf.concat([h14, h1_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        h16 = tf.nn.conv2d_transpose(h14, self.G_W16, output_shape=[batch_size, 128, 128, self.ch_G16], strides=[1, 2, 2, 1])  \n",
    "\n",
    "        h16 = tf.nn.tanh(h16)\n",
    "\n",
    "\n",
    "        return h16\n",
    "\n",
    "    \n",
    "    \n",
    "    # Network Parameters\n",
    "\n",
    "    def __init__(self, sess, batch_size):\n",
    "\n",
    "        self.learning_rate = 0.00001  \n",
    "\n",
    "\n",
    "\n",
    "        self.sess = sess\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.keep_prob = 0.5\n",
    "\n",
    "        self.input_image_shape = [image_height, image_width, 1] \n",
    "        self.input_condition_shape = [1,1,condition_shape] \n",
    "        self.output_image_shape = [image_height, image_width, 1]\n",
    "\n",
    "        self.l1_weight = 1000.0 \n",
    "\n",
    "\n",
    "\n",
    "        '''channels'''\n",
    "\n",
    "        # Gen_Encoding\n",
    "\n",
    "        self.ch_G0 = 1\n",
    "\n",
    "        self.ch_G1 = 64-condition_shape \n",
    "        \n",
    "        self.ch_G2 = 128\n",
    "\n",
    "        self.ch_G3 = 256\n",
    "\n",
    "        self.ch_G4 = 512\n",
    "\n",
    "        self.ch_G5 = 512\n",
    "\n",
    "        self.ch_G6 = 512\n",
    "\n",
    "        self.ch_G7 = 512\n",
    "        \n",
    "        self.ch_condition = condition_shape\n",
    "\n",
    "\n",
    "        # Gen_Decoding\n",
    "\n",
    "        self.ch_G9 = 512\n",
    "\n",
    "        self.ch_G10 = 512\n",
    "\n",
    "        self.ch_G11 = 512\n",
    "\n",
    "        self.ch_G12 = 256\n",
    "\n",
    "        self.ch_G13 = 128\n",
    "        \n",
    "        self.ch_G14 = 64\n",
    "\n",
    "        self.ch_G16 = 1\n",
    "\n",
    "        # Discrim\n",
    "\n",
    "        self.ch_D0 = 2+condition_shape \n",
    "\n",
    "        self.ch_D1 = 64\n",
    "\n",
    "        self.ch_D2 = 128\n",
    "\n",
    "        self.ch_D3 = 256\n",
    "\n",
    "        self.ch_D4 = 512\n",
    "\n",
    "        self.ch_D5 = 1\n",
    "\n",
    "\n",
    "\n",
    "        '''parameters'''\n",
    "\n",
    "        # Gen_encoding\n",
    "\n",
    "        self.G_W1 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G0, self.ch_G1], stddev=0.02), name=\"G_W1\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W2 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G1+7, self.ch_G2], stddev=0.02), name='G_W2')\n",
    "\n",
    "        self.G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W3 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G2, self.ch_G3], stddev=0.02), name='G_W3')\n",
    "\n",
    "        self.G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W4 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G3, self.ch_G4], stddev=0.02), name='G_W4')\n",
    "\n",
    "        self.G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W5 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G4, self.ch_G5], stddev=0.02), name='G_W5')\n",
    "\n",
    "        self.G_bn5 = batch_norm(name=\"G_bn5\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W6 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G5, self.ch_G6], stddev=0.02), name='G_W6')\n",
    "\n",
    "        self.G_bn6 = batch_norm(name=\"G_bn6\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W7 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G6, self.ch_G7], stddev=0.02), name='G_W7')\n",
    "\n",
    "        self.G_bn7 = batch_norm(name=\"G_bn7\")\n",
    "\n",
    "\n",
    "\n",
    "        # Gen_Decoding\n",
    "\n",
    "        self.G_W9 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G9, self.ch_G7], stddev=0.02), name='G_W9')\n",
    "\n",
    "        self.G_bn9 = batch_norm(name=\"G_bn9\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W10 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G10, self.ch_G9 + self.ch_G6], stddev=0.02), name='G_W10')\n",
    "\n",
    "        self.G_bn10 = batch_norm(name=\"G_bn10\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W11 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G11, self.ch_G10 + self.ch_G5], stddev=0.02), name='G_W11')\n",
    "\n",
    "        self.G_bn11 = batch_norm(name=\"G_bn11\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W12 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G12, self.ch_G11 + self.ch_G4], stddev=0.02), name='G_W12')\n",
    "\n",
    "        self.G_bn12 = batch_norm(name=\"G_bn12\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W13 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G13, self.ch_G12 + self.ch_G3], stddev=0.02), name='G_W13')\n",
    "\n",
    "        self.G_bn13 = batch_norm(name=\"G_bn13\")\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W14 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G14, self.ch_G13 + self.ch_G2], stddev=0.02), name='G_W14')\n",
    "\n",
    "        self.G_bn14 = batch_norm(name=\"G_bn14\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.G_W16 = tf.Variable(tf.truncated_normal([4, 4, self.ch_G16, self.ch_G14 + self.ch_G1], stddev=0.02), name='G_W16')\n",
    "\n",
    "\n",
    "\n",
    "        # Discrim\n",
    "\n",
    "        self.D_W1 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D0, self.ch_D1], stddev=0.02), name='D_W1')\n",
    "\n",
    "        self.D_bn1 = batch_norm(name=\"D_bn1\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W2 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D1, self.ch_D2], stddev=0.02), name='D_W2')\n",
    "\n",
    "        self.D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W3 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D2, self.ch_D3], stddev=0.02), name='D_W3')\n",
    "\n",
    "        self.D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W4 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D3, self.ch_D4], stddev=0.02), name='D_W4')\n",
    "\n",
    "        self.D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "\n",
    "\n",
    "        self.D_W5 = tf.Variable(tf.truncated_normal([4, 4, self.ch_D4, self.ch_D5], stddev=0.02), name='D_W5')\n",
    "\n",
    "\n",
    "\n",
    "        self.gen_params = [\n",
    "\n",
    "            self.G_W1,\n",
    "\n",
    "            self.G_W2,\n",
    "\n",
    "            self.G_W3,\n",
    "\n",
    "            self.G_W4,\n",
    "\n",
    "            self.G_W5,\n",
    "\n",
    "            self.G_W6,\n",
    "\n",
    "            self.G_W7,\n",
    "\n",
    "            self.G_W9,\n",
    "\n",
    "            self.G_W10,\n",
    "\n",
    "            self.G_W11,\n",
    "\n",
    "            self.G_W12,\n",
    "\n",
    "            self.G_W13,\n",
    "\n",
    "            self.G_W14,\n",
    "\n",
    "            self.G_W16\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        self.discrim_params = [\n",
    "\n",
    "            self.D_W1,\n",
    "\n",
    "            self.D_W2,\n",
    "\n",
    "            self.D_W3,\n",
    "\n",
    "            self.D_W4,\n",
    "\n",
    "            self.D_W5\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "\n",
    "    def discriminate(self, input_img, target, condition):\n",
    "        \n",
    "\n",
    "        dummy = np.ones((1,image_height,image_width,1))\n",
    "        if condition_shape==7:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy,condition[0,0,0,4]*dummy,condition[0,0,0,5]*dummy,condition[0,0,0,6]*dummy], axis=3)\n",
    "        if condition_shape==6:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy,condition[0,0,0,4]*dummy,condition[0,0,0,5]*dummy], axis=3)\n",
    "        if condition_shape==4:\n",
    "            img_concat = tf.concat([input_img, target, condition[0,0,0,0]*dummy, condition[0,0,0,1]*dummy,condition[0,0,0,2]*dummy,condition[0,0,0,3]*dummy], axis=3)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        h1 = tf.nn.conv2d(img_concat, self.D_W1, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h1 = self.D_bn1(h1)\n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.D_W2, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h2 = self.D_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.D_W3, strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "        h3 = self.D_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.pad(h3, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')  \n",
    "\n",
    "        h4 = tf.nn.conv2d(h4, self.D_W4, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "        h4 = self.D_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.pad(h4, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')  \n",
    "\n",
    "        h5 = tf.nn.conv2d(h5, self.D_W5, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "        h5 = tf.nn.sigmoid(h5)\n",
    "\n",
    "\n",
    "\n",
    "        return h5\n",
    "\n",
    "\n",
    "\n",
    "    # Method for generating the fake images\n",
    "\n",
    "    def sample_generator(self, input_image, con_input, batch_size=1):\n",
    "\n",
    "        input_img = tf.placeholder(tf.float32, [batch_size] + self.input_image_shape)\n",
    "        con_input = tf.placeholder(tf.float32, [batch_size] + self.input_condition_shape)\n",
    "\n",
    "\n",
    "\n",
    "        h1 = h1_ = tf.nn.conv2d(input_img, self.G_W1, strides=[1, 2, 2, 1], padding='SAME')  # [?,256,256,3] -> [?,128,128,64]\n",
    "\n",
    "        h1 = lrelu(h1)\n",
    "\n",
    "\n",
    "\n",
    "        h2 = tf.nn.conv2d(h1, self.G_W2, strides=[1, 2, 2, 1], padding='SAME')  # [?,128,128,64] -> [?,64,64,128]\n",
    "\n",
    "        h2 = h2_ = self.G_bn2(h2)\n",
    "\n",
    "        h2 = lrelu(h2)\n",
    "\n",
    "\n",
    "\n",
    "        h3 = tf.nn.conv2d(h2, self.G_W3, strides=[1, 2, 2, 1], padding='SAME')  # [?,64,64,128] -> [?,32,32,256]\n",
    "\n",
    "        h3 = h3_ = self.G_bn3(h3)\n",
    "\n",
    "        h3 = lrelu(h3)\n",
    "\n",
    "\n",
    "\n",
    "        h4 = tf.nn.conv2d(h3, self.G_W4, strides=[1, 2, 2, 1], padding='SAME')  # [?,32,32,256] -> [?,16,16,512]\n",
    "\n",
    "        h4 = h4_ = self.G_bn4(h4)\n",
    "\n",
    "        h4 = lrelu(h4)\n",
    "\n",
    "\n",
    "\n",
    "        h5 = tf.nn.conv2d(h4, self.G_W5, strides=[1, 2, 2, 1], padding='SAME')  # [?,16,16,512] -> [?,8,8,512]\n",
    "\n",
    "        h5 = h5_ = self.G_bn5(h5)\n",
    "\n",
    "        h5 = lrelu(h5)\n",
    "\n",
    "\n",
    "\n",
    "        h6 = tf.nn.conv2d(h5, self.G_W6, strides=[1, 2, 2, 1], padding='SAME')  # [?,8,8,512] -> [?,4,4,512]\n",
    "\n",
    "        h6 = h6_ = self.G_bn6(h6)\n",
    "\n",
    "        h6 = lrelu(h6)\n",
    "\n",
    "\n",
    "\n",
    "        h7 = tf.nn.conv2d(h6, self.G_W7, strides=[1, 2, 2, 1], padding='SAME')  # [?,4,4,512] -> [?,2,2,512]\n",
    "\n",
    "        h7 = h7_ = self.G_bn7(h7)\n",
    "\n",
    "        h7 = lrelu(h7)\n",
    "        \n",
    "        h7 = tf.concat([h7, con_input], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h9 = tf.nn.conv2d_transpose(h7, self.G_W9, output_shape=[batch_size, 2, 2, self.ch_G9], strides=[1, 2, 2, 1])  # [?,1,1,512] -> [?,2,2,512]\n",
    "\n",
    "        h9 = tf.nn.dropout(self.G_bn9(h9), keep_prob=self.keep_prob)\n",
    "\n",
    "        h9 = tf.nn.relu(h9)\n",
    "\n",
    "        h9 = tf.concat([h9, h6_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h10 = tf.nn.conv2d_transpose(h9, self.G_W10, output_shape=[batch_size, 4, 4, self.ch_G10], strides=[1, 2, 2, 1])  # [?,2,2,512+512] -> [?,4,4,512]\n",
    "\n",
    "        h10 = tf.nn.dropout(self.G_bn10(h10), keep_prob=self.keep_prob)\n",
    "\n",
    "        h10 = tf.nn.relu(h10)\n",
    "\n",
    "        h10 = tf.concat([h10, h5_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h11 = tf.nn.conv2d_transpose(h10, self.G_W11, output_shape=[batch_size, 8, 8, self.ch_G11], strides=[1, 2, 2, 1])  # [?,4,4,512+512] -> [?,8,8,512]\n",
    "\n",
    "        h11 = tf.nn.dropout(self.G_bn11(h11), keep_prob=self.keep_prob)\n",
    "\n",
    "        h11 = tf.nn.relu(h11)\n",
    "\n",
    "        h11 = tf.concat([h11, h4_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h12 = tf.nn.conv2d_transpose(h11, self.G_W12, output_shape=[batch_size, 16, 16, self.ch_G12], strides=[1, 2, 2, 1])  # [?,8,8,512+512] -> [?,16,16,512]\n",
    "\n",
    "        h12 = self.G_bn12(h12)\n",
    "\n",
    "        h12 = tf.nn.relu(h12)\n",
    "\n",
    "        h12 = tf.concat([h12, h3_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h13 = tf.nn.conv2d_transpose(h12, self.G_W13, output_shape=[batch_size, 32, 32, self.ch_G13], strides=[1, 2, 2, 1])  # [?,16,16,512+512] -> [?,32,32,256]\n",
    "\n",
    "        h13 = self.G_bn13(h13)\n",
    "\n",
    "        h13 = tf.nn.relu(h13)\n",
    "\n",
    "        h13 = tf.concat([h13, h2_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        h14 = tf.nn.conv2d_transpose(h13, self.G_W14, output_shape=[batch_size, 64, 64, self.ch_G14], strides=[1, 2, 2, 1])  # [?,32,32,256+256] -> [?,64,64,128]\n",
    "\n",
    "        h14 = self.G_bn14(h14)\n",
    "\n",
    "        h14 = tf.nn.relu(h14)\n",
    "\n",
    "        h14 = tf.concat([h14, h1_], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        h16 = tf.nn.conv2d_transpose(h14, self.G_W16, output_shape=[batch_size, 128, 128, self.ch_G16], strides=[1, 2, 2, 1])  # [?,128,128,64+64] -> [?,256,256,3]\n",
    "\n",
    "        h16 = tf.nn.tanh(h16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        generated_samples = self.sess.run(h16, feed_dict={input_img: input_image, con_input: con_input})\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return generated_samples\n",
    "\n",
    "\n",
    "\n",
    "    # Train Generator and return the loss\n",
    "\n",
    "    def train_gen(self, input_img, target_img, con_input):\n",
    "\n",
    "        \n",
    "        _, loss_val_GAN, loss_val_L1 = self.sess.run([self.train_op_gen, self.G_loss_GAN, self.G_loss_L1], feed_dict={self.input_img: input_img, self.target_img: target_img, self.input_con: con_input})\n",
    "\n",
    "\n",
    "        return loss_val_GAN, loss_val_L1\n",
    "\n",
    "\n",
    "\n",
    "    # Train Discriminator and return the loss\n",
    "\n",
    "    def train_discrim(self, input_img, target_img, input_condition):\n",
    "\n",
    "        _, loss_val_D = self.sess.run([self.train_op_discrim, self.D_loss], feed_dict={self.input_img: input_img, self.target_img: target_img, self.input_con: input_condition})\n",
    "\n",
    "        return loss_val_D\n",
    "    \n",
    "\n",
    "    \n",
    "    # Build the Network\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        self.input_con = tf.placeholder(tf.float32, [self.batch_size] + self.input_condition_shape)\n",
    "        self.input_img = tf.placeholder(tf.float32, [self.batch_size] + self.input_image_shape)\n",
    "        \n",
    "\n",
    "        self.target_img = tf.placeholder(tf.float32, [self.batch_size] + self.output_image_shape)\n",
    "\n",
    "\n",
    "        gen_img = self.generate(self.input_img, self.input_con)\n",
    "\n",
    "\n",
    "\n",
    "        d_real = self.discriminate(self.input_img, self.target_img, self.input_con)\n",
    "\n",
    "        d_fake = self.discriminate(self.input_img, gen_img, self.input_con)\n",
    "\n",
    "\n",
    "\n",
    "        self.D_loss = tf.reduce_mean(tf.square(1-d_real) + tf.square(d_fake))\n",
    "        \n",
    "\n",
    "        self.G_loss_GAN = tf.reduce_mean(tf.square(1-d_fake))\n",
    "        \n",
    "        self.G_loss_L1 = tf.reduce_mean(tf.abs(self.target_img - gen_img))\n",
    "\n",
    "        self.G_loss = self.G_loss_GAN + self.G_loss_L1 * self.l1_weight\n",
    "\n",
    "       \n",
    "        self.train_op_discrim = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5).minimize(self.D_loss, var_list=self.discrim_params)\n",
    "\n",
    "        self.train_op_gen = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5).minimize(self.G_loss, var_list=self.gen_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-335eaf92f7c8>:31: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training data -> dataset \n",
    "train_input_path = []\n",
    "train_output_path = []\n",
    "train_condition_path = []\n",
    "\n",
    "features_list = os.listdir(train_input_dir)\n",
    "for feature_name in features_list :\n",
    "    x = '%s/%s' %(train_input_dir, feature_name)\n",
    "    train_input_path.append(x)\n",
    "    \n",
    "labels_list = os.listdir(train_output_dir)\n",
    "for label_name in labels_list :\n",
    "    x = '%s/%s' %(train_output_dir, label_name)\n",
    "    train_output_path.append(x)\n",
    "    \n",
    "condition_list = os.listdir(train_condition_dir)\n",
    "for condition_name in condition_list :\n",
    "    x = '%s/%s' %(train_condition_dir, condition_name)\n",
    "    train_condition_path.append(x)\n",
    "\n",
    "def read_path(inputpath, outputpath, conditionpath):\n",
    "\n",
    "    x = np.load(inputpath.decode(), allow_pickle = True) \n",
    "    y = np.load(outputpath.decode(), allow_pickle = True)\n",
    "    z = np.load(conditionpath.decode(), allow_pickle = True)\n",
    "    return x.astype(np.float64), y.astype(np.float64), z.astype(np.float64)\n",
    "    \n",
    "features, labels, conditions = (train_input_path, train_output_path, train_condition_path)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels, conditions))\n",
    "dataset = dataset.map(lambda features, labels, conditions: tuple(tf.py_func(read_path, [features,labels, conditions], [tf.float64, tf.float64, tf.float64])))\n",
    "dataset = dataset.batch(batch_size=batch_size).repeat(count=total_epoch).shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "feature, label, condition = iter.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\young\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-4ec23679cdaf>:129: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-4331c9fa6a3c>:25: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./output/checkpoint\\model_epoch010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4331c9fa6a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mimg_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_condition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mimg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mimg_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_epoch = tf.Variable(0, trainable=False, name='global_step')\n",
    "global_epoch_increase = tf.assign(global_epoch, tf.add(global_epoch, 1))\n",
    "\n",
    "\n",
    "result_dir =  './output/result'\n",
    "ckpt_dir = './output/checkpoint'\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "\n",
    "    \n",
    "sess = tf.Session()\n",
    "\n",
    "model = Pix2Pix(sess, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epoch = sess.run(global_epoch)\n",
    "\n",
    "while True:\n",
    "\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        saver.save(sess, ckpt_dir + '/model_epoch'+str(epoch).zfill(3))\n",
    "    if epoch % 5 ==0:\n",
    "        saver.save(sess, ckpt_dir + '/model_epoch'+str(epoch).zfill(3))\n",
    "    if epoch == total_epoch:\n",
    "        break\n",
    "    for step in range(total_batch):\n",
    "        img_input, img_target, img_condition = sess.run([feature, label, condition])\n",
    "        img_input = np.reshape(img_input,[1,128,128,1]) \n",
    "        img_target = np.reshape(img_target,[1,128,128,1])\n",
    "        \n",
    "        \n",
    "        if step % 200 == 0: #DvsG train ratio\n",
    "            loss_GAN, loss_L1 = model.train_gen(img_input, img_target, img_condition)  # Train Generator and get the loss value\n",
    "            loss_D = model.train_discrim(img_input, img_target, img_condition)         # Train Discriminator and get the loss value\n",
    "        else :\n",
    "            loss_GAN, loss_L1 = model.train_gen(img_input, img_target, img_condition)  # Train Generator and get the loss value\n",
    "                        \n",
    "                    \n",
    "        if step % training_iters == 0:\n",
    "            print('Epoch: [', epoch, '/', total_epoch, '], ', 'Step: [', step, '/', total_batch, '], D_loss: ', loss_D, ', G_loss_GAN: ', loss_GAN, ', G_loss_L1: ', loss_L1)\n",
    "            \n",
    "       \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    epoch = sess.run(global_epoch_increase)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
